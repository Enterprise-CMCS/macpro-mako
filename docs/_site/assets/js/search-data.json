{"0": {
    "doc": "Access Requests",
    "title": "Access Requests",
    "content": "You’ll need access to a few systems to be a fully privileged developer. This section will guide you in making the necessary access requests. Note: Account access should be your first step in onboarding, as the requests can take time to complete. ",
    "url": "/docs/onboarding/access-requests/",
    
    "relUrl": "/docs/onboarding/access-requests/"
  },"1": {
    "doc": "Access Requests",
    "title": "Table of contents",
    "content": ". | Git organization | Git Repository | AWS and Cloud VPN | Zscaler VPN | Snyk | Code Climate | . ",
    "url": "/docs/onboarding/access-requests/#table-of-contents",
    
    "relUrl": "/docs/onboarding/access-requests/#table-of-contents"
  },"2": {
    "doc": "Access Requests",
    "title": "Git organization",
    "content": "To be a fully privileged developer on the macpro-mako project, you will need access to both the GitHub organization and the repository. Access to the GitHub organization is governed by a CMS team and CMS Job Codes. Please follow the instructions to obtain org access. ",
    "url": "/docs/onboarding/access-requests/#git-organization",
    
    "relUrl": "/docs/onboarding/access-requests/#git-organization"
  },"3": {
    "doc": "Access Requests",
    "title": "Git Repository",
    "content": "To be a fully privileged developer on the macpro-mako project, you will need access to both the GitHub organization and the repository. To be granted access to the repo:, please send an email to bpaige@gswell.com which includes: . | Name | GitHub user id | Level of access requested (read, write, admin, maintain) | Reason for access / who you are | . ",
    "url": "/docs/onboarding/access-requests/#git-repository",
    
    "relUrl": "/docs/onboarding/access-requests/#git-repository"
  },"4": {
    "doc": "Access Requests",
    "title": "AWS and Cloud VPN",
    "content": "The macpro-mako project is deployed to a designated set of three AWS accounts. While many workflows can be done without direct AWS Console/CLI access, a fully equipped developer will need AWS access. Further, accessing the AWS Console or CLI requires fetching temporary credentials from a service called Kion. This service is behind the CMS Cloud VPN. So, to have full AWS access, you will need two things: access to the Cloud VPN so you can hit Kion, and then permissions for the project’s specific AWS accounts. CMS requests to be granted access to these two systems can be made together, or seperately. Please follow this how-to guide to obtain access to AWS and the Cloud VPN. ",
    "url": "/docs/onboarding/access-requests/#aws-and-cloud-vpn",
    
    "relUrl": "/docs/onboarding/access-requests/#aws-and-cloud-vpn"
  },"5": {
    "doc": "Access Requests",
    "title": "Zscaler VPN",
    "content": "This project communicates with Seatool. Sometimes, during development, access to the Seatool frontend is helpful. To gain access to it, you must first get access to the Zscaler VPN. Please follow these instructions to gain access to Zscaler. ",
    "url": "/docs/onboarding/access-requests/#zscaler-vpn",
    
    "relUrl": "/docs/onboarding/access-requests/#zscaler-vpn"
  },"6": {
    "doc": "Access Requests",
    "title": "Snyk",
    "content": "Snyk is a software tool that specializes in identifying and resolving security vulnerabilities in code dependencies and performing static code analysis. It scans project dependencies for known security issues and provides recommendations for fixing them, enhancing code security. CMS has a Snyk installation that our project ties into. It can be found https://snyk.cms.gov/. Access is governed by a CMS job code, as well as Snyk permissions. Please follow these steps to gain access to Snyk: . | Request / Confirm EUA job code: ENT_APPSEC_TOOLS: Access to Enterprise Application Security Tools: Snyk | Login to Enterprise User Administration (EUA) | Select View My Identity | Select the “Job Codes” tab | Review your current Job Codes. Do you have ENT_APPSEC_TOOLS? | If no, request job code ENT_APPSEC_TOOLS . | On the “Task” sidebar select “Modify My Job Codes | Be sure to select the “*Confirmation (Required) check box | Select Next | At the bottom left of the page select “Add a Job Code” | Enter the job code you want to add, In this case ENT_APPSEC_TOOLS and select “Search” | Check the select box and click the “Select” bottom. | Click “Next” | Enter a “Justification Reason” and select “Finish” | Example: I am a CMS contractor, requesting Access to Enterprise Application Security Tools: Snyk order to support development and maintenance of the suite of MACPRO systems supported by Fearless and it’s sub-contractors under Primary Contract Number: GS-35F-115GA:75FCMC22F0093: | Now wait for the request to be approved. You need ENT_APPSEC_TOOLS before the cloud team will setup your access. | . | Let me know when each user has confirmed that you have the ENT_APPSEC_TOOLS the Job Code and I will send a Snyk invite the email address associated with their EUA ID. | . ",
    "url": "/docs/onboarding/access-requests/#snyk",
    
    "relUrl": "/docs/onboarding/access-requests/#snyk"
  },"7": {
    "doc": "Access Requests",
    "title": "Code Climate",
    "content": "We use Code Climate to monitor project quality. This includes running maintainability checks for Pull Requests, which flags code that doesn’t meet best practices. Checks include function length, file length, cognitive complexity, and duplication. Code Climate is a completely external tool which is free to use. You may go to Code Climate and create a new account. We recommend you sign up with GitHub, for convenience, so you won’t need to maintain a separate username and password. Once you have an account, you may view any repositories for which you have access. For private repositories, you will not be able to view the repository in Code Climate until you have Git repository write access (see above). ",
    "url": "/docs/onboarding/access-requests/#code-climate",
    
    "relUrl": "/docs/onboarding/access-requests/#code-climate"
  },"8": {
    "doc": "alerts",
    "title": "alerts",
    "content": "Summary . The alerts service deploys a Simple Notification Service (SNS) topic to REGION_A. This topic can be leveraged by any other service for sending alerts. Detail . | To subscribe an email, phone number, or something else to the topic, find the SNS topic using the AWS Console and add the subscription manually. | No SNS subscriptions are made by the deployment process. The topic is created, and several other services are configured to publish notifications to the topic, but the topic itself is not automatically configured to fan out any notifications. Here’s why: . | Since dev environments may receive many notifications due to failures related to development, and since those notifications can be noisy, we likely never want to automatically subscribe to dev environments’ SNS topics. | We likely only want to subscribe to notifications for higher/long running environments like main, val, and production. | Manually adding the subscription to higher environments was judged to be low effort, as it’s a one-time operation. | After adding an email as a subscriber to SNS, the email must be confirmed by clicking a link in a confirmation email. This added to the decision to handle subscriptions manually, as a human would need to verify the email manually even if the subscription was made automatically. | . | . ",
    "url": "/docs/services/alerts/",
    
    "relUrl": "/docs/services/alerts/"
  },"9": {
    "doc": "api",
    "title": "api",
    "content": " ",
    "url": "/docs/services/api/",
    
    "relUrl": "/docs/services/api/"
  },"10": {
    "doc": "api",
    "title": "Summary",
    "content": "The api service deploys a lambda-backed API Gateway that is used by the frontend to interact with the data layer. Access to any of its endpoints is guarded at a high level by AWS Cognito, ensuring only authenticated users may reach it. The lambda functions that back each endpoint enforce further fine-grain access according to business rules. ",
    "url": "/docs/services/api/#summary",
    
    "relUrl": "/docs/services/api/#summary"
  },"11": {
    "doc": "api",
    "title": "Detail",
    "content": "The largest component of the api service is the API Gateway itself. This is a standard deployment of a regional, REST API Gateway. We do not apply custom certificates or DNS names to the api gateway endpoint (yet); instead, our application uses the amazon generated SSL endpoint. There are four endpoints on the api. Each is guarded by AWS IAM, meaning that while the API Gateway is publicly available, the API will not forward your request to the backing lambda unless you provide valid credentials obtained through AWS Cognito. This way, only users with an account that we can authenticate may successfully call endpoints. The four endpoints are: . | /search (POST): This endpoint accepts search queries from clients in the form of OpenSearch Query DSL queries. Once the query is received, the lambda adds extra query filters to ensure fine grain auth. This works by looking up the user making the call in Cognito, determining what type of user (cms or state) is making the call, determining what states that user has access to (if appropriate), and modifying the query in a way that will only return results for those states. By design, the only thing the search endpoint adds is related to authentication; the rest of the query building is left to the frontend for faster and more flexible development. | /item (POST): The item endpoint is used to fetch details for exactly one record. While you can form a query to do this and use the search endpoint, the item endpoint is for convenience. Simply make a post call containing the ID of the desired record to the item endpoint, and the record will be returned. Note that fine grain auth is still enforced in an identical way to search, whereby you will only obtain results for that ID if you should have access to that ID. | /getAttachmentUrl (POST): This endpoint is used to generate a presigned url for direct client downloading of S3 data, enforcing fine grain auth along the way. This is how we securely allow download of submission attachment data. From the details page, a user may click a file to download. Once clicked, their client makes a post to /getAttachmentUrl with the attachment metadata. The lambda function determines if the caller should or should not have access based on identical logic as the other endpoints (the UI would not display something they cannot download, but this guards against bad actors). If access is allowed, the lambda function generates a presigned url good for 60 seconds and returns it to the client browser, at which point files are downloaded automatically. | /allForms (GET): This endpoint serves GET requests and will return a list off all available webforms and their associated version. the result will look like: { ABP1: [ ‘1’, ‘2’ ], ABP2: [ ‘1’ ] } | /forms (GET): This endpoint function serves as the backend for handling forms and their associated data. This function provides various features, including retrieving form data, validating access, and serving the requested form content. The request to this endpoint must include a formId in the request body. Optionally, you can include a formVersion parameter. If you access this endpoint with formId without specifying formVersion, it will return the latest version. Form schemas are stored in a Lambda layer. Each form is organized in its directory, and each version is stored within that directory. The Lambda layer is located in the “opt” directory when deployed to aws. To access a specific version of a form with a formId, use the following URL structure: /opt/${formId}/v${formVersion}.js. The form schemas are versioned and stored in Git under the “api/layers” directory. | . All endpoints and backing functions interact with the OpenSearch data layer. As such, and because OpenSearch is deployed within a VPC, all lambda functions of the api service are VPC based. The functions share a security group that allows outbound traffic. All function share an IAM role. This is for convenicence; we can do one role per function if we find that valuable. The permissions include: . | OpenSearch permissions to allow access to the data layer | Cognito permissions to look up user attributes; allows for enforcement of fine grain auth. | AssumeRole permissions for a very specific cross account role, which is required to generate the presigned urls for the legacy OneMac data. | . ",
    "url": "/docs/services/api/#detail",
    
    "relUrl": "/docs/services/api/#detail"
  },"12": {
    "doc": "auth",
    "title": "auth",
    "content": " ",
    "url": "/docs/services/auth/",
    
    "relUrl": "/docs/services/auth/"
  },"13": {
    "doc": "auth",
    "title": "Summary",
    "content": "The auth service builds the infrastructure for our authentication and authorization solution: Amazon Cognito. A user pool and identity pool is deployed, and may conditionally be pointed to IDM (external identity provider). ",
    "url": "/docs/services/auth/#summary",
    
    "relUrl": "/docs/services/auth/#summary"
  },"14": {
    "doc": "auth",
    "title": "Detail",
    "content": "The core of the api service is a cognito user pool and identity pool, which work together to provide an auth solution: . | user pool: this is the user directory, where all active users and their attributes are stored. | This is where we specify the user attribute schema, informed by but not beholden to IDM. | The attribute schema is difficiult to update, and often requires deleting the user pool. This is acceptable for two reasons. One, updating the attribute schema would be a rare event. Two, since in higher environments all users are federated, the user pool itself holds no unique data; as such, it is safe to delete and simply rebuild without having data loss. | . | identity pool: this is associated with the user pool, and allows us to grant certain AWS permissions to authenticated and/or unauthenticated entities. | authenticated users may assume a role that gives them permissions to invoke the api gateway, as well as see information about their own cognito user. | unauthenticated user may assume a role that gives them no permissions. | . | . In the near future, higher environments will configure IDM as an external identity provider. Ephemeral/dev environments will continue to use only the cognito user pool. ",
    "url": "/docs/services/auth/#detail",
    
    "relUrl": "/docs/services/auth/#detail"
  },"15": {
    "doc": "Auth",
    "title": "Authentication and Authorization",
    "content": "The macpro-mako project caters to both CMS and State users, each requiring specific permissions to ensure secure access and compliance with CMS policies. This page outlines the design and hierarchy of roles within our system, focusing on the allocation of permissions rather than the underlying authentication technology. ",
    "url": "/docs/design/authentication_and_authorization/#authentication-and-authorization",
    
    "relUrl": "/docs/design/authentication_and_authorization/#authentication-and-authorization"
  },"16": {
    "doc": "Auth",
    "title": "Detail",
    "content": "We have seven distinct user roles within our system, categorized into application roles (app roles) and IDM roles. App Roles include: . | onemac-micro-statesubmitter | onemac-micro-readonly | onemac-micro-reviewer | onemac-micro-helpdesk | . These roles authorize privileges within the OneMAC Micro application. Importantly, a user can be assigned only one of these roles at any given time. These roles are defined within IDM by the user’s custom attribute custom:cms-roles and are essential for operational functionality within the application. IDM Roles consist of: . | onemac-micro-sysadmin | onemac-micro-roleapprover | onemac-micro-statesysadmin | . IDM roles grant privileges within IDM itself, focusing on role request approvals rather than direct application access. A user can hold one of these roles at any given time, with specific restrictions regarding their combination: . | Users cannot hold a mix of state and CMS IDM roles. For example, if a user holds the onemac-micro-statesysadmin role, they cannot hold a CMS IDM role. | Conversely, if a user has a CMS IDM role, they cannot hold the onemac-micro-statesysadmin role. However, a user could hold both CMS IDM roles (onemac-micro-sysadmin and onemac-micro-roleapprover) simultaneously. | . Authentication vs. Authorization: . | Authentication: Anyone with an active IDM account can log in to the OneMAC Micro application. If they lack one of the four app roles, they will be informed via a banner that they need to request a role through IDM. | Authorization: Only users with an app role are authorized to perform actions within the application. | . Moreover, it is possible for a user to have one app role and one or more IDM roles concurrently. This setup allows for users who manage IDM’s administrative tasks to also use the application, provided they have the necessary app role. In conclusion, while any IDM account holder can access the OneMAC Micro application, only those with an app role are granted operational privileges. IDM roles serve administrative functions within IDM and have specific combination restrictions to ensure clear role delineation. ",
    "url": "/docs/design/authentication_and_authorization/#detail",
    
    "relUrl": "/docs/design/authentication_and_authorization/#detail"
  },"17": {
    "doc": "Auth",
    "title": "Auth",
    "content": " ",
    "url": "/docs/design/authentication_and_authorization/",
    
    "relUrl": "/docs/design/authentication_and_authorization/"
  },"18": {
    "doc": "Jira Issue Commenter",
    "title": "Jira Issue Commenter",
    "content": "Automatically links Pull Requests to Jira Issues mentioned in the PR body. ",
    "url": "/docs/workflows/auto-create-jira-comment/",
    
    "relUrl": "/docs/workflows/auto-create-jira-comment/"
  },"19": {
    "doc": "Jira Issue Commenter",
    "title": "Summary",
    "content": "The macpro-mako project uses GitHub Pull Requests to review and merge and code change. A GitHub pull request is a feature that allows developers to propose changes to a project’s codebase. When a developer wants to suggest changes to a project, they create a pull request which includes the code changes they’ve made. The pull request then allows other developers to review the proposed changes, discuss any potential issues, and ultimately merge the changes into the main codebase. The macpro-mako project uses Jira to plan, schedule, and track development work items. As a general rule, most pull requests should be related to a Jira Issue. In fact our PR template has a section where you may list related issues. The auto-create-jira-comment workflow is meant to scan pull requests for Jira Issue links; any issues that it finds receives a new comment “This issue was referenced on (link to pull request)”. If it finds no issue links in the PR, nothing happens. If it finds one, two, or ‘n’ issues, they all receive that same “This issue was referenced…” comment. While this workflow will not automatically close issues in Jira, it works to create that link between work item and pull request, provided the team can add Jira Issue links to PRs. ",
    "url": "/docs/workflows/auto-create-jira-comment/#summary",
    
    "relUrl": "/docs/workflows/auto-create-jira-comment/#summary"
  },"20": {
    "doc": "Jira Issue Commenter",
    "title": "Configuration, Notes, YSK",
    "content": "Set JIRA_USERNAME and JIRA_TOKEN as github secrets . The workflow file expects two secrets to be set, JIRA_USERNAME and JIRA_TOKEN. IF they’re not set, the workflow will not fail, but it will be unable to comment on Jira issues. You’ll need to create username and token secrets for a jira user. The token is more accurately called a Personal Access Token (PAT) in Jira, and can be created by logging into Jira in a web browser and following these instructions. On MACPro, we use a service user; you probably should, too. If you’re on MACPro, you may be able to leverage our existing service user; reach out to bpaige@gswell.com or Nathan O’Donnell about possible access. Load these values for JIRA_USERNAME and JIRA_TOKEN into the repository’s actions secrets, and the workflow functionality will be operational. Review/Update the JIRA_BASE_URL in the workflow file . In the workflow definition, there is a hardcoded value for JIRA_BASE_URL. This is used to more precisely find Jira issue links. As this value is the same for MACPro projects, it was hardcoded to reduce configuration burden. But if your project uses a different Jira than the one listed, update this value to your Jira base url. ",
    "url": "/docs/workflows/auto-create-jira-comment/#configuration-notes-ysk",
    
    "relUrl": "/docs/workflows/auto-create-jira-comment/#configuration-notes-ysk"
  },"21": {
    "doc": "AWS Login",
    "title": "AWS Login",
    "content": "Authenticating to an AWS account(s) is a required first step for many workflows. ",
    "url": "/docs/developer-guide/aws-auth/",
    
    "relUrl": "/docs/developer-guide/aws-auth/"
  },"22": {
    "doc": "AWS Login",
    "title": "Table of contents",
    "content": ". | AWS Console Login . | Summary | Prerequisites | Procedure | Notes | . | AWS CLI credentials . | Summary | Prerequisites | Procedure | Notes | . | . AWS Console Login . Summary . This procedure will take you to the AWS Console in a web browser, for one of the AWS accounts used by this project. Prerequisites . | Completed all onboarding | . Procedure . To get to the AWS Console: . | Login to the cloud VPN, https://cloudvpn.cms.gov | Go to the CMS Kion (Cloudtamer) site. This is a great link to bookmark. Note: if the Kion site fails to load in your browser, it is very likely an issue with your VPN. The Kion site is only accessibly while actively on the VPN. | Login with your CMS EUA credentials. | Select the drop down menu next to the appropriate account. | Select Cloud Access Roles | Select the role you wish to assume. | Select Web Access. The AWS Console for the account should open in a new browser tab. Once the console is open, you may close your VPN connection, if you wish. | . Notes . | Once connected to the AWS Console, you can close your VPN connection if you’d like. The VPN is only needed when authenticating to Kion and gaining AWS credentials. | Your browser session is valid for up to 4 hours. After 4 hours, you will need to redo this procedure. | . AWS CLI credentials . Summary . This procedure will show you how to retrieve AWS CLI credentils for one of the AWS accounts used by this project, granting you programmatic access to AWS. This is required for any operations you may run directly against AWS. Prerequisites . | Completed all onboarding | . Procedure . | Login to the cloud VPN, https://cloudvpn.cms.gov | Go to the CMS Kion (Cloudtamer) site. This is a great link to bookmark. Note: if the Kion site fails to load in your browser, it is very likely an issue with your VPN. The Kion site is only accessibly while actively on the VPN. | Login with your CMS EUA credentials. | Select the drop down menu next to the appropriate account. | Select Cloud Access Roles | Select the role you wish to assume. | Select ‘Short-term Access Keys’. | Click the code block under ‘Option 1’, to copy the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_SESSION_TOKEN environment variables to your clipboard. | Navigate to a terminal on your mac, and paste the credentials. You should now be able to interact with AWS programmatically, based on the role you selected in Kion. | . Notes . | There are three available options when getting access keys from Kion. The instructions above detail Option 1, which is essentially copying and pasting env variables to a terminal. Feel free to use one of the other options if you’d prefer. For sake of simplicity, Option 1 will be the only one documented and supported here. | Once you have credentials from Kion, you can close your VPN connection if you’d like. The VPN is only required when talking to Kion to obtain credentials. | The credentials are valid for 4 hours, after which you’ll need to redo this procedure. | . ",
    "url": "/docs/developer-guide/aws-auth/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/aws-auth/#table-of-contents"
  },"23": {
    "doc": "Update from Base",
    "title": "Update from Base",
    "content": "How to update your project with the latest macpro-base-template releases. ",
    "url": "/docs/developer-guide/base-update/",
    
    "relUrl": "/docs/developer-guide/base-update/"
  },"24": {
    "doc": "Update from Base",
    "title": "Table of contents",
    "content": ". | Update from Base . | Summary | Prerequisites: | Procedure | Notes | . | . Update from Base . Summary . This command fetches the latest macpro-base-template changes and merges them into your branch. You may then resolve any merge conflicts, and open a PR. Prerequisites: . | Completed all onboarding | . Procedure . | Use the run script: nvm use run base-update . | . Notes . | None | . ",
    "url": "/docs/developer-guide/base-update/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/base-update/#table-of-contents"
  },"25": {
    "doc": "Code Style Cheatsheet",
    "title": "Code Style",
    "content": "What is it? . A set of rules or guidelines used when writing the source code for a computer program. Why is it? . It keeps the code looking neat and tidy, so anyone on the team can jump in and not get lost in a jungle of curly braces and indentation levels. It’s like everyone speaking the same slang in a super cool secret club. Plus, it saves time from arguing over trivial stuff, like whether tabs are better than spaces, so there’s more time for the fun stuff—coding and creating! . ",
    "url": "/docs/team-norms/code-style/#code-style",
    
    "relUrl": "/docs/team-norms/code-style/#code-style"
  },"26": {
    "doc": "Code Style Cheatsheet",
    "title": "OneMAC Style Norms",
    "content": " ",
    "url": "/docs/team-norms/code-style/#onemac-style-norms",
    
    "relUrl": "/docs/team-norms/code-style/#onemac-style-norms"
  },"27": {
    "doc": "Code Style Cheatsheet",
    "title": "Cheatsheet",
    "content": "TL;DR? No worries, here’s a cheatsheet of the concepts outlined below: . | DO NOT destructure so we maintain object context for methods and properties used in code | . Object Access . When integrating with complex objects, consider maintaining the object’s integrity rather than opting for destructuring. This approach ensures that the object’s context is preserved, enhancing readability and maintainability. Nomenclature simplification . For instance, rather than breaking down the object into individual variables, which can lead to verbose and confusing naming conventions, maintain the object as a whole. ```typescript jsx const { setModalOpen, setContent: setModalContent, setOnAccept: setModalOnAccept, } = useModalContext(); const { setContent: setBannerContent, setBannerShow, setBannerDisplayOn, } = useAlertContext(); . // vs . const modal = useModalContext(); const alert = useAlertContext(); . #### Usage implication This method simplifies reference to its properties and methods, providing a clearer and more direct understanding of its usage within the code. This strategy is particularly beneficial in scenarios where the object's structure and context significantly contribute to its functionality and meaning in the application. ```typescript jsx &lt;form onSubmit={form.handleSubmit(async (data) =&gt; { try { await submit({ //... }); alert.setContent({ header: \"RAI response submitted\", body: `The RAI response for ${item._source.id} has been submitted.`, }); alert.setBannerShow(true); alert.setBannerDisplayOn(\"/dashboard\"); navigate({ path: \"/dashboard\" }); } catch (e) { //... } })} &gt; . ",
    "url": "/docs/team-norms/code-style/#cheatsheet",
    
    "relUrl": "/docs/team-norms/code-style/#cheatsheet"
  },"28": {
    "doc": "Code Style Cheatsheet",
    "title": "Code Style Cheatsheet",
    "content": " ",
    "url": "/docs/team-norms/code-style/",
    
    "relUrl": "/docs/team-norms/code-style/"
  },"29": {
    "doc": "Communication",
    "title": "Communication",
    "content": "Here’s how our team communicates. Note: Account access should be your first step in onboarding, as the requests can take time to complete. ",
    "url": "/docs/onboarding/communication/",
    
    "relUrl": "/docs/onboarding/communication/"
  },"30": {
    "doc": "Communication",
    "title": "Table of contents",
    "content": ". | Recurring Meetings | Slack | Jira | GitHub Pull Requests (PRs) | Email | . ",
    "url": "/docs/onboarding/communication/#table-of-contents",
    
    "relUrl": "/docs/onboarding/communication/#table-of-contents"
  },"31": {
    "doc": "Communication",
    "title": "Recurring Meetings",
    "content": "Here’s a list of recurring meetings you might need. If you need or want an invite, reach out on Slack. | Meeting | Day(s) | Time | Link | . | Standup | Daily | 11:30am - 12:00pm ET | Bookmarked in Slack Channel. | . | Touchpoint | Wed | 10:00am - 10:30am ET | Ask for the Hangouts invite on Slack. | . ",
    "url": "/docs/onboarding/communication/#recurring-meetings",
    
    "relUrl": "/docs/onboarding/communication/#recurring-meetings"
  },"32": {
    "doc": "Communication",
    "title": "Slack",
    "content": "We love Slack. Our primary means of synchronous and/or ad hoc communication is our project Slack channel. All developers and stakeholders should join the channel, and feel free to put any kind of project related information in it. Questions, comments, discussions about failing builds, “is anyone available to help me?”, “i’m going to be out for a few hours”, “check out this cool new thing I found”, “good morning everyone”, etc. can all go in the Slack channel. Please note that comments related to Pull Requests and Issues are best made on those objects. For example: putting a comment about a Pull Request in Slack is generally inappropriate, as the Pull Request is then not a complete record of what occurred. However, and please remember this: do what you feel is best. Discretion is what allows us to move fast, so don’t be afraid to break the guidelines. ",
    "url": "/docs/onboarding/communication/#slack",
    
    "relUrl": "/docs/onboarding/communication/#slack"
  },"33": {
    "doc": "Communication",
    "title": "Jira",
    "content": "We use Jira to document work items. This should be the place where ideas are written down, where acceptance criteria is developed, and where questions/comments/concerns pertaining to the work should take place. Specifically, you probably don’t want to comment on a work item or its content in Slack, as it’s very valuable to have the entire development process on record in the Jira Task/Story. So, put work item related content in the Task as much as possible. ",
    "url": "/docs/onboarding/communication/#jira",
    
    "relUrl": "/docs/onboarding/communication/#jira"
  },"34": {
    "doc": "Communication",
    "title": "GitHub Pull Requests (PRs)",
    "content": "GitHub Pull Requests are the primary vehicle to propose code changes to macpro-mako. A PR is always used to ship code to the main branch, with very few exceptions. This project has PR templates which will be used automatically when you create a PR. While the template sets up a PR with the fields that are typically required, you need to add the content to the various sections. When authoring a PR, you typically want to be as descriptive as possible. The goal is to write a PR as a complete description of the changeset, so that the PR and any underlying Issue can completely communicate the changes being made to someone otherwise unfamiliar with the work. We want to be able to look back at the PR in a month, six months, or years on, and be able to fully understand what was changed and why. Your audience for PRs are your peers, non technical stakeholders, and your future self. High quality PRs are a hallmark of successful and maintainable projects; don’t go short on them. ",
    "url": "/docs/onboarding/communication/#github-pull-requests-prs",
    
    "relUrl": "/docs/onboarding/communication/#github-pull-requests-prs"
  },"35": {
    "doc": "Communication",
    "title": "Email",
    "content": "We don’t like email. However, it is used for access requests, onboarding, communication with external entities, and/or for any other communication that’s difficult to facilitate another way. Don’t hesitate to send an email if there’s not a better way (do what you think is best), but avoid it by rule and use it by exception. ",
    "url": "/docs/onboarding/communication/#email",
    
    "relUrl": "/docs/onboarding/communication/#email"
  },"36": {
    "doc": "dashboard",
    "title": "dashboard",
    "content": " ",
    "url": "/docs/services/dashboard/",
    
    "relUrl": "/docs/services/dashboard/"
  },"37": {
    "doc": "dashboard",
    "title": "Why do I need this?",
    "content": "Part of any good project is a way to determine how well it is working. The purpose of a CloudWatch Dashboard is to determine the performance, health, and a variety of other aspects that factor into the product being delivered. What we have done here is provided an easy to use solution that will make creating a dashboard easy and deploying it even easier. ",
    "url": "/docs/services/dashboard/#why-do-i-need-this",
    
    "relUrl": "/docs/services/dashboard/#why-do-i-need-this"
  },"38": {
    "doc": "dashboard",
    "title": "Quick Disclaimer",
    "content": "In order to add the dashboard to existing projects it is important to note that is relies on consistant namespacing across aws services. It must be able to distinguish things such as the project and branch name for example. ",
    "url": "/docs/services/dashboard/#quick-disclaimer",
    
    "relUrl": "/docs/services/dashboard/#quick-disclaimer"
  },"39": {
    "doc": "dashboard",
    "title": "Getting Started",
    "content": "In order to use the CloudWatch Dashboard you must bring over the dashboard folder which is located in the src/services/ directory of this repo. Where this folder gets added is entirely dependant upon the structure of the project it’s being added to, but the good news is that this service has no dependencies on other services (meaning it is standalone). ",
    "url": "/docs/services/dashboard/#getting-started",
    
    "relUrl": "/docs/services/dashboard/#getting-started"
  },"40": {
    "doc": "dashboard",
    "title": "Making edits to the Dashboard",
    "content": "Once the dashboard is deployed to the AWS account it can be found in the CloudWatch Dashboards section by the name of ${stage-name}-dashboard. Edits can be made to this dashboard and when edits are complete simply save the dashboard and then click on the generate template button. The contents in here are what the templateDashboard.txt file should consist of. A simple copy, paste, and commit later and the changes are now ready to be deployed to higher environments. ",
    "url": "/docs/services/dashboard/#making-edits-to-the-dashboard",
    
    "relUrl": "/docs/services/dashboard/#making-edits-to-the-dashboard"
  },"41": {
    "doc": "data",
    "title": "data",
    "content": " ",
    "url": "/docs/services/data/",
    
    "relUrl": "/docs/services/data/"
  },"42": {
    "doc": "data",
    "title": "Summary",
    "content": "The data service deploys our OpenSearch data layer and supporting infrastructure. ",
    "url": "/docs/services/data/#summary",
    
    "relUrl": "/docs/services/data/#summary"
  },"43": {
    "doc": "data",
    "title": "Detail",
    "content": "OpenSearch, Amazon’s managed Elasticsearch offering, was selected as the data layer tech. ",
    "url": "/docs/services/data/#detail",
    
    "relUrl": "/docs/services/data/#detail"
  },"44": {
    "doc": "Deploy",
    "title": "Deploy",
    "content": "Deploys the stage . ",
    "url": "/docs/workflows/deploy/",
    
    "relUrl": "/docs/workflows/deploy/"
  },"45": {
    "doc": "Deploy",
    "title": "Summary",
    "content": "This GitHub workflow deploys an application to AWS and performs various tests and checks on the resources deployed. It consists of several jobs: . | init: This job validates the name of the branch and ensures that it adheres to the naming convention used by the Serverless Framework. | deploy: This job deploys the application to AWS using the Serverless Framework. It checks out the source code, configures AWS credentials, and deploys the application using the run deploy command. | test: This job runs automated tests on the deployed application. It checks out the source code, configures AWS credentials, and runs the tests using the run test command. | cfn-nag: This job performs a static analysis of the AWS CloudFormation templates used by the application. It checks out the source code, configures AWS credentials, and uses the stelligent/cfn_nag action to analyze the templates. | resources: This job retrieves information about the resources deployed to AWS by the application. It checks out the source code, configures AWS credentials, and uses the aws cloudformation list-stack-resources command to retrieve information about the resources deployed. | release: This job creates a release of the application. It checks out the source code, configures AWS credentials, and creates a GitHub release with the artifacts produced by the test, cfn-nag, and resources jobs. | . ",
    "url": "/docs/workflows/deploy/#summary",
    
    "relUrl": "/docs/workflows/deploy/#summary"
  },"46": {
    "doc": "Deploy",
    "title": "Workflow Details",
    "content": ". | Name: Deploy | Triggers: This workflow is triggered on every push to any branch, except for branches that start with “skipci”. | Concurrency: This workflow is limited to one concurrent run per branch. | Environment Variables: . | STAGE_NAME: The name of the deployment stage. This is set to the name of the branch by default. | . | Permissions: . | id-token: write | contents: write | issues: write | pull-requests: write | . | Jobs: . | init: . | Runs on: Ubuntu 20.04 | Steps: | . | Validate the stage name | . | deploy: . | Runs on: Ubuntu 20.04 | Needs: init | Environment: STAGE_NAME | Steps: | . | Checkout the source code | Use the aws-actions/configure-aws-credentials action to configure AWS credentials | Deploy the application using the run deploy command | . | test: . | Runs on: Ubuntu 20.04 | Needs: deploy | Environment: STAGE_NAME | Steps: | . | Checkout the source code | Use the aws-actions/configure-aws-credentials action to configure AWS credentials | Run automated tests using the run test command | . | cfn-nag: . | Runs on: Ubuntu 20.04 | Needs: deploy | Environment: STAGE_NAME | Steps: | . | Checkout the source code | Use the aws-actions/configure-aws-credentials action to configure AWS credentials | Use the stelligent/cfn_nag action to perform a static analysis of the AWS CloudFormation templates | . | resources: . | Runs on: Ubuntu 20.04 | Needs: deploy | ** | . | . | . ",
    "url": "/docs/workflows/deploy/#workflow-details",
    
    "relUrl": "/docs/workflows/deploy/#workflow-details"
  },"47": {
    "doc": "Deploy a Stage",
    "title": "Deploy a Stage",
    "content": "How-to deploy a new or existing stage to AWS. ",
    "url": "/docs/developer-guide/deploy/",
    
    "relUrl": "/docs/developer-guide/deploy/"
  },"48": {
    "doc": "Deploy a Stage",
    "title": "Table of contents",
    "content": ". | Deploy a stage . | Summary | Prerequisites: | Procedure | Notes | . | Deploy an individual service . | Description | Prerequisites: | Procedure | Notes | . | Deploy using GitHub Actions . | Summary | Prerequisites: | Procedure | Notes | . | . Deploy a stage . Summary . This deploys the entire application, so the entire stage, to AWS. Prerequisites: . | Completed all onboarding | . Procedure . | Obtain and set AWS CLI credentials | Deploy using the run script: cd macpro-mako nvm use run deploy --stage foo . | . Notes . | None | . Deploy an individual service . Description . This will deploy a single service for a given stage. All other services on which your target service is dependent must already be deployed for the stage. For example: if service B depends on service A, and you want to use this procedure to deploy only service B, then service A must have already been deployed. Prerequisites: . | Completed all onboarding | . Procedure . | Obtain and set AWS CLI credentials | Deploy using the run script: cd macpro-mako nvm use run deploy --service bar --stage foo . | . Notes . | None | . Deploy using GitHub Actions . Summary . This project uses GitHub Actions as its CI/CD tool. For the most part, this project also adheres to GitOps. That said… . Each branch pushed to the macpro-mako git repository is automatically deployed to AWS. GitHub Actions sees the ‘push’ event of a new branch, and runs our Deploy.yml workflow. After a few minutes, the branch will be fully deployed. This 1:1 relationship between git branches and deployed stages is the reason why ‘stage’ and ‘branch’ are sometimes used interchangeably to refer to a deployed set of the application. Prerequisites: . | Git repo write access; complete the Git access request portion of onboarding | . Procedure . | Obtain and set AWS CLI credentials | Create a new branch based off of any other branch or commit. The ‘main’ branch is the most common branch from which to create new branches, and is shown in the following procedure.: cd macpro-mako git checkout main git pull git checkout -b foo git push --set-upstream origin foo . | Monitor the status of your branch’s deployment in the repo’s Actions area. | . Notes . | None | . ",
    "url": "/docs/developer-guide/deploy/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/deploy/#table-of-contents"
  },"49": {
    "doc": "Design",
    "title": "Design",
    "content": "This is a place where you can find more about the design and history of different pieces of the application. The macpro-mako project is a serverless monorepo. It is, for the most part, a collection of standalone Serverless Framework micro services bound together in a repository. Loose coupling of the micro services is facilitated using one or several tools, which include CloudFormation outputs, AWS Systems Manager Parameter Store paramters, and AWS Secrets Manager stores. This section will describe each service in a high level of detail. ",
    "url": "/docs/design",
    
    "relUrl": "/docs/design"
  },"50": {
    "doc": "Destroy a Stage",
    "title": "Destroy a Stage",
    "content": "How-to destroy a stage in AWS. ",
    "url": "/docs/developer-guide/destroy/",
    
    "relUrl": "/docs/developer-guide/destroy/"
  },"51": {
    "doc": "Destroy a Stage",
    "title": "Table of contents",
    "content": ". | Destroy using GitHub Actions - branch deletion . | Summary | Prerequisites | Procedure | Notes | . | Destroy using GitHub Actions - manual dispatch . | Summary | Prerequisites | Procedure | Notes | . | Destroy a stage . | Summary | Prerequisites: | Procedure | Notes | . | Destroy an individual service . | Summary | Prerequisites: | Procedure | Notes | . | . Destroy using GitHub Actions - branch deletion . Summary . GitHub Actions is usually the best way to destroy a stage. A Destroy workflow exists for this project, which will neatly take down any and all infrastructure related to a branch/stage, as well as deactivate the GitHub Environment, if it exists. In most cases, stages are deployed from a branch in the git repo. If this is the case, and if the branch can be safely deleted, destroying using GitHub Actions and branch deletion is the preferred approach. Prerequisites . | Git repo write access; complete the Git access request portion of onboarding | . Procedure . | Stop and think about what you are doing. Destroying is a lot easier to avoid then to undo. | Delete the branch for the stage you wish to delete. cd macpro-mako git push --delete origin foo . | Monitor the status of your stage’s destruction in the repo’s Actions area. | . Notes . | None | . Destroy using GitHub Actions - manual dispatch . Summary . The same GitHub Actions workflow referenced above can be triggered manually. This is primarily useful if there is AWS infrastructure that still exists for a branch that has been deleted, and you don’t want to go to the trouble of running destroy from your Mac. Or, if you want to do a clean deploy of a stage, but you don’t want to delete the branch, this can also be handy. Prerequisites . | Git repo write access; complete the Git access request portion of onboarding | . Procedure . | In a browser, go to the repo | Click the Actions tab | Click Destroy, located on the left hand side of the screen. | Click ‘Run workflow’ . | Leave ‘Use workflow from’ set to main. | Enter the name of the stage you wish to destroy in the free text field. | Click ‘Run workflow’ | . | Monitor the status of your stage’s destruction in the repo’s Actions area. | . Notes . | None | . Destroy a stage . Summary . This destroys an entire application, so the entire stage, to AWS. Prerequisites: . | Completed all onboarding | . Procedure . | Stop and think about what you are doing. Destroying is a lot easier to avoid then to undo. | Obtain and set AWS CLI credentials | Destroy using the run script: cd macpro-mako nvm use run destroy --stage foo . | . Notes . | After running the above destroy command, the script will output any Cloudformation stacks that will be deleted, and ask you to verify the stage name to proceed with destruction. If you’d like to proceed, re-enter the stage name and hit enter. | The destroy script will hold your terminal process open until all stacks report as DESTROY_COMPLETE in cloudformation. If a stack fails to delete, or if there is a timeout, the script will fail. You may retry the script again, but it may be worth investigating the failure. | Please be mindful of what you are doing. | . Destroy an individual service . Summary . This will destroy a single service for a given stage. Prerequisites: . | Completed all onboarding | . Procedure . | Stop and think about what you are doing. Destroying is a lot easier to avoid then to undo. | Obtain and set AWS CLI credentials | Destroy a single service using the run script: cd macpro-mako nvm use run destroy --service bar --stage foo . | . Notes . | All notes from the Destroy a Stage section (above) hold true for destroying an individual service. | . ",
    "url": "/docs/developer-guide/destroy/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/destroy/#table-of-contents"
  },"52": {
    "doc": "Dev (DORA) Metrics",
    "title": "DORA Metrics",
    "content": " ",
    "url": "/docs/developer-metrics/#dora-metrics",
    
    "relUrl": "/docs/developer-metrics/#dora-metrics"
  },"53": {
    "doc": "Dev (DORA) Metrics",
    "title": "Table of contents",
    "content": ". | Purpose of this view | What is DORA | How does it work | . ClICK HERE to view current DORA metrics for the macpro-mako project. ",
    "url": "/docs/developer-metrics/#table-of-contents",
    
    "relUrl": "/docs/developer-metrics/#table-of-contents"
  },"54": {
    "doc": "Dev (DORA) Metrics",
    "title": "Purpose of this view",
    "content": "The goal of the DORA metrics view is to provide a pane of glass view into the effieiency of ‘dev’ teams along with the quality of code that they produce. These metrics and the questions that they ask is based in large part around DORA. ",
    "url": "/docs/developer-metrics/#purpose-of-this-view",
    
    "relUrl": "/docs/developer-metrics/#purpose-of-this-view"
  },"55": {
    "doc": "Dev (DORA) Metrics",
    "title": "What is DORA",
    "content": "DORA, short for (DevOps Research and Assessment) was a study performed by google into determining a way rate the performance and efficiency. The below article does a great job explaining in detail. View Article . ",
    "url": "/docs/developer-metrics/#what-is-dora",
    
    "relUrl": "/docs/developer-metrics/#what-is-dora"
  },"56": {
    "doc": "Dev (DORA) Metrics",
    "title": "How does it work",
    "content": "Linked below is Development Metrics dashboard in question. But in essence it collects a data around deployments, commits, and pull-requests that occur in github and displays it in a useful and easily readable way. ",
    "url": "/docs/developer-metrics/#how-does-it-work",
    
    "relUrl": "/docs/developer-metrics/#how-does-it-work"
  },"57": {
    "doc": "Dev (DORA) Metrics",
    "title": "Dev (DORA) Metrics",
    "content": " ",
    "url": "/docs/developer-metrics/",
    
    "relUrl": "/docs/developer-metrics/"
  },"58": {
    "doc": "Developer Guide",
    "title": "Developer Guide",
    "content": "How to do common things as a developer on the project . Let this serve as a running list of practical, operational how-to guides for developers. If something is missing, please add it! Thanks. ",
    "url": "/docs/developer-guide",
    
    "relUrl": "/docs/developer-guide"
  },"59": {
    "doc": "Run Docs Site Locally",
    "title": "Run Jekyll Docs Site Locally",
    "content": "How-to run our GitHub Pages Jekyll docs site (the site you’re viewing) locally. ",
    "url": "/docs/developer-guide/docs/#run-jekyll-docs-site-locally",
    
    "relUrl": "/docs/developer-guide/docs/#run-jekyll-docs-site-locally"
  },"60": {
    "doc": "Run Docs Site Locally",
    "title": "Table of contents",
    "content": ". | Start the docs site locally . | Summary | Prerequisites: | Procedure | Notes | . | . Start the docs site locally . Summary . This procedure will walk you through starting up the docs site locally. You may want to run the docs site locally if you are modifying the documentation, allowing you to preview before committing. Prerequisites: . | Completed all onboarding | You will need a docker runtime up and running. This can be Docker Desktop (if licensed), or an open source tool like Colima. Generally, if you can run the command docker ps and not get an error, you have a docker runtime up and running. | . Procedure . | Start the docs site using the run script: cd macpro-mako run docs . | In a browser, visit http://localhost:4000 to view the running site. | As you make changes to the files under the repo’s docs folder, you should see the changes reflected in your browser in less than a second. | . Notes . ",
    "url": "/docs/developer-guide/docs/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/docs/#table-of-contents"
  },"61": {
    "doc": "Run Docs Site Locally",
    "title": "Run Docs Site Locally",
    "content": " ",
    "url": "/docs/developer-guide/docs/",
    
    "relUrl": "/docs/developer-guide/docs/"
  },"62": {
    "doc": "email",
    "title": "email",
    "content": " ",
    "url": "/docs/services/email/",
    
    "relUrl": "/docs/services/email/"
  },"63": {
    "doc": "email",
    "title": "Summary",
    "content": "The email service deploys the lambdas, SNS topics, and Configuration Sets needed to send email. ",
    "url": "/docs/services/email/#summary",
    
    "relUrl": "/docs/services/email/#summary"
  },"64": {
    "doc": "email",
    "title": "Detail",
    "content": "AWS SES is an account-wide service for basic sending and receiving of email. By creating lambdas to build the emails and sending the email with a branch-specific configuration set, we can follow the events of email sending and take action based on those events. Secrets Manager . The workflow will not successfully deploy unless the emailAddressLookup object is defined: . Named {project}/default/emailAddressLookup or {project}/{stage}/emailAddressLookup { “sourceEmail”:”\"CMS MACPro no-reply\" spa-reply@cms.hhs.gov”, “osgEmail”:”\"OSG\" mako.stateuser@gmail.com”, “chipInbox”:”\"CHIP Inbox\" mako.stateuser@gmail.com”, “chipCcList”:”\"CHIP CC 1\" mako.stateuser@gmail.com;\"CHIP CC 2\" mako.stateuser@gmail.com”, “dpoEmail”:”\"DPO Action\" mako.stateuser@gmail.com”, “dmcoEmail”:”\"DMCO Action\" mako.stateuser@gmail.com”, “dhcbsooEmail”:”\"DHCBSOO Action\" mako.stateuser@gmail.com” } . These values are set during deployment as environment variables on the lambda. You can edit these values in the AWS Console on the Lambda configuration tab. LAUCH NOTE!!! The defined email addresses have been stored as om/default/emailAddressLookup in the production account, with om/production/emailAddressLookup overwriting those email addresses with the test email addresses. Delete the om/production/emailAddressLookup before the real launch deploy (you can also edit the environment variables after the lambda is built). Test accounts . There are gmail accounts created to facilitate email testing. Please contact a MACPro team member for access to these inboxes. At this time, there is only one available email inbox. | mako.stateuser@gmail.com - a state user account - does have an associated OneMAC login | . Templates . The email services uses the serverless-ses-template plugin to manage the email templates being used for each stage. To edit the templates, edit index.js in ./ses-email-templates. Each template configuration object requires: . | name: the template name (note, the stage name is appended to this during deployment so branch templates remain unique to that stage). At this time, the naming standard for email templates is based on the event details. Specifically, the action and the authority values from the decoded event. If action is not included in the event data, “new-submission” is assumed. | subject: the subject line of the email, may contain replacement values using . | html: the email body in html, may contain replacement values using . | text: the email body in text, may contain replacement values using . | . ",
    "url": "/docs/services/email/#detail",
    
    "relUrl": "/docs/services/email/#detail"
  },"65": {
    "doc": "email",
    "title": "Email Sending Service with AWS CDK",
    "content": "This guide provides an overview and implementation of a robust email sending service using AWS Cloud Development Kit (CDK). The service includes features such as dedicated IP pools, configuration sets, verified email identities, and monitoring through SNS topics. ",
    "url": "/docs/services/email/#email-sending-service-with-aws-cdk",
    
    "relUrl": "/docs/services/email/#email-sending-service-with-aws-cdk"
  },"66": {
    "doc": "Home",
    "title": "MAKO",
    "content": "A new project by the CMS MACPRO platform team. Get started now View it on GitHub . ",
    "url": "/#mako",
    
    "relUrl": "/#mako"
  },"67": {
    "doc": "Home",
    "title": "Welcome!",
    "content": "The macpro-mako project, a.k.a. MAKO, a.k.a. Micro, is a redesign of MACPRO Onemac. The mission to be a modern submission and review portal for select CMS data remains the same, but the architecture is different in some important ways. This documentation site is a WIP and currently shows outdated information, as the project hasn’t quite reached the ‘norming’ phase yet. Thank you for your patience. ",
    "url": "/#welcome",
    
    "relUrl": "/#welcome"
  },"68": {
    "doc": "Home",
    "title": "About the project",
    "content": "The macpro-mako project is a work of the Centers for Medicare &amp; Medicaid Services (CMS). Thank you to the contributors of macpro-mako! . ",
    "url": "/#about-the-project",
    
    "relUrl": "/#about-the-project"
  },"69": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"70": {
    "doc": "JIRA Workflow",
    "title": "JIRA Workflow",
    "content": " ",
    "url": "/docs/team-norms/jira-workflow/",
    
    "relUrl": "/docs/team-norms/jira-workflow/"
  },"71": {
    "doc": "JIRA Workflow",
    "title": "Introduction",
    "content": "This document outlines our team’s process for managing tasks using the JIRA Kanban board. It’s designed to provide clarity and consistency on how we track and progress through work items. ",
    "url": "/docs/team-norms/jira-workflow/#introduction",
    
    "relUrl": "/docs/team-norms/jira-workflow/#introduction"
  },"72": {
    "doc": "JIRA Workflow",
    "title": "Kanban Columns",
    "content": "Backlog . | Population: The backlog is populated with new tasks during the planning phase. | Prioritization: Tasks are prioritized based on urgency and importance. | . Ready . | Preparation: Tasks are moved to “Ready” once they are clearly defined and ready for development. | Final Checks: Ensure that all necessary information and acceptance criteria are present. Story points should be added prior to moving to Ready. | . In Progress . | Active Development: When a task is actively being worked on, it is moved to “In Progress”. | Daily Updates: Developers should provide daily updates or comments on the task to indicate progress. | . In Review . | Code Review: Tasks are moved here when the development is complete and they are awaiting code review. | Peer Feedback: Team members provide feedback and approval. If feedback requires a significant modification or rewrite to the code, the ticket should be moved back to In Progress. | . In Testing . | Quality Assurance: Tasks in this column are undergoing thorough testing by the QA team. | Env URL: When a developer moves a ticket into In Testing, the dev should make a comment on the ticket that includes the deployed environment’s URL, as well as probably tagging the QA team for convenience. | Hands Off: Developers should not push code that updates environments for work that is In Testing, without coordinating with the QA team. This is to prevent deployments interfering with the QA process. | Bug Reporting: Any issues discovered during testing are reported and linked to the task. | Failures: If a ticket fails QA for reasons that should not be addressed separately (like a bug), the QA team will move the ticket back to In Review. | . Ready for Merge . | Final Approval: Once testing is complete and the task has passed all checks, it moves to “Ready for Merge”. | Merge Preparation: The task is prepared for merging into the main codebase. | . In Pipeline . | Deployment: Tasks here have been merged to master, and are in the process of being verified on master by the QA team. | Monitoring: Close monitoring of the feature in the live environment for any immediate issues. | . Done . | Completion: Tasks are moved to “Done” when they are merged to master and verified on master, if applicable. | Review: The team may review completed tasks to ensure all objectives were met. | Demo Coordination: If the completed work is going to be demoed, coordinate a time with Product to relay the context of the feature and how to demonstrate it. | . ",
    "url": "/docs/team-norms/jira-workflow/#kanban-columns",
    
    "relUrl": "/docs/team-norms/jira-workflow/#kanban-columns"
  },"73": {
    "doc": "JIRA Workflow",
    "title": "Sprint Tracking",
    "content": "Current and Upcoming Sprints . | Active Sprint: Tasks currently being worked on are tracked under the active sprint, e.g., “Sprint 2.4”. | Future Sprints: Upcoming work items are assigned to future sprints, and periodically rescheduled during refinement as events unfold. | . Sprint Review . | Regular Reviews: At the end of each sprint, the team reviews progress and adjusts future plans accordingly. | Continuous Improvement: Lessons learned are discussed and processes are adjusted to improve future sprints. | . ",
    "url": "/docs/team-norms/jira-workflow/#sprint-tracking",
    
    "relUrl": "/docs/team-norms/jira-workflow/#sprint-tracking"
  },"74": {
    "doc": "JIRA Workflow",
    "title": "Responsibility and Accountability",
    "content": ". | Ownership: Team members take ownership of tasks they are working on and update their status accordingly. | Collaboration: The entire team collaborates to ensure tasks move smoothly through the workflow. | . By adhering to this JIRA workflow, we aim to maintain a high level of organization and efficiency within our development process. ",
    "url": "/docs/team-norms/jira-workflow/#responsibility-and-accountability",
    
    "relUrl": "/docs/team-norms/jira-workflow/#responsibility-and-accountability"
  },"75": {
    "doc": "Launch Darkly",
    "title": "Launch Darkly",
    "content": "LaunchDarkly is a feature management platform that enables software teams to effectively manage and control the lifecycle of features in their applications. It provides feature flagging, experimentation, and continuous delivery capabilities, allowing teams to release features with confidence and control. ",
    "url": "/docs/developer-guide/launch-darkly/",
    
    "relUrl": "/docs/developer-guide/launch-darkly/"
  },"76": {
    "doc": "Launch Darkly",
    "title": "Table of contents",
    "content": ". | AWS Console Login . | Summary | Prerequisites | Procedure | Notes | . | . AWS Console Login . Summary . These instructions will help you get started with Launch Darkly. Prerequisites . | Completed all onboarding | . Procedure . To get to the AWS Console: . | Login to the cloud VPN, https://cloudvpn.cms.gov | Follow these instructions to get access to the Launch Darkly client LaunchDarkly Guide. This is a great link to bookmark. Your EUA account will need the correct job codes before being able to access the LaunchDarkly client. | Once you have access to LaunchDarkly you will need to check if you have the correct user roles to access all three environments for MACPRO_MAKO. If not, Keith Littlejohn is our CMS LaunchDarkly point of contact. | . Notes . | Once you have access to LaunchDarkly you can switch on feature flags which should show/hide various features. | Access codes for UI, API, and mobile all live in AWS System Parameter Store. They were ddistrubted by Keith Littlejohn. We do not have access to the environment’s api keys. If we need to rotate them please reach out to him. | There is three environments in LaunchDarkly for Mako: Dev, IMPL, and Production which point to our three environments. Dev is used for our ephemeral environments. | . ",
    "url": "/docs/developer-guide/launch-darkly/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/launch-darkly/#table-of-contents"
  },"77": {
    "doc": "List Running Stages",
    "title": "List Running Stages",
    "content": "How to get a list of currently running stages for this project in the current AWS account. ",
    "url": "/docs/developer-guide/list-running-stages/",
    
    "relUrl": "/docs/developer-guide/list-running-stages/"
  },"78": {
    "doc": "List Running Stages",
    "title": "Table of contents",
    "content": ". | List Running Stages . | Summary | Prerequisites: | Procedure | Notes | . | Run Report using GitHub Actions . | Summary | Prerequisites: | Procedure | Notes | . | . List Running Stages . Summary . This returns a list of currently running stages for this project in the current AWS account. Prerequisites: . | Completed all onboarding | . Procedure . | Obtain and set AWS CLI credentials | Use the run script: nvm use run listRunningStages . | . Notes . | None | . Run Report using GitHub Actions . Summary . This project uses GitHub Actions as its CI/CD tool. Each of our repositories has a GitHub Actions workflow added to run this list running stages command and report the results to slack on a schedule. This workflow may also be manually invoked. Prerequisites: . | Git repo access; complete the Git access request portion of onboarding | Access to CMS slack to see the generated report. | . Procedure . | Browse to the actions page of the repository in GitHub, select the “Running Stage Notifier” workflow and press run workflow. | . Notes . | None | . ",
    "url": "/docs/developer-guide/list-running-stages/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/list-running-stages/#table-of-contents"
  },"79": {
    "doc": "Onboarding",
    "title": "Onboarding",
    "content": "Welcome to the team! . This section exists to be your guide through the macpro-mako project onboarding process. We’ve done our best to make this as linear and painless as possible… let us know how we did! If you find any gaps in the documentation, inaccurate information, or need help, please reach out via Slack or via email to bpaige@gswell.com. ",
    "url": "/docs/onboarding",
    
    "relUrl": "/docs/onboarding"
  },"80": {
    "doc": "Overview",
    "title": "Overview",
    "content": "The 10,000ft view . ",
    "url": "/docs/overview/",
    
    "relUrl": "/docs/overview/"
  },"81": {
    "doc": "Overview",
    "title": "Table of contents",
    "content": ". | Overview | Architecture | Development Metrics (DORA) | AWS Resources | . ",
    "url": "/docs/overview/#table-of-contents",
    
    "relUrl": "/docs/overview/#table-of-contents"
  },"82": {
    "doc": "Overview",
    "title": "Overview",
    "content": "The macpro-mako project is a redesign of MACPRO Onemac. The mission to be a modern submission and review portal for select CMS data remains the same, but the architecture is different in some important ways. ",
    "url": "/docs/overview/",
    
    "relUrl": "/docs/overview/"
  },"83": {
    "doc": "Overview",
    "title": "Architecture",
    "content": "A diagram is often the best way to communicate the architecture: . ",
    "url": "/docs/overview/#architecture",
    
    "relUrl": "/docs/overview/#architecture"
  },"84": {
    "doc": "Overview",
    "title": "Development Metrics (DORA)",
    "content": "We programmatically publish a set of Development metrics that align to the DevOps Research and Assesment (DORA) standards. Those metrics can be viewed here. ",
    "url": "/docs/overview/#development-metrics-dora",
    
    "relUrl": "/docs/overview/#development-metrics-dora"
  },"85": {
    "doc": "Overview",
    "title": "AWS Resources",
    "content": "You can view and download a list of all aws resources this project uses for higher environments here. ",
    "url": "/docs/overview/#aws-resources",
    
    "relUrl": "/docs/overview/#aws-resources"
  },"86": {
    "doc": "Pre-Commit Usage",
    "title": "Pre-Commit Usage",
    "content": "How to use pre-commit on the project . ",
    "url": "/docs/developer-guide/pre-commit/",
    
    "relUrl": "/docs/developer-guide/pre-commit/"
  },"87": {
    "doc": "Pre-Commit Usage",
    "title": "Table of contents",
    "content": ". | Context | Installation | Configuration | . Context . Pre-commit is a python package that enables projects to specifies a list of hooks to run before a commit is made (a pre-commit hook). This is really useful to enforce standards or conventions, as it prevents non conformant changes from getting committed. On this project, we use pre-commit to automate several checks, including: . | running a code formatting check based on prettier | checking for large files typically not desired to keep in source control | scanning for secret material, such as AWS keys | . Aside from these checks being run prior to any commit being pushed, they are also run by a GitHub Actions workflow when a pull request is made. Installation . Good news! If you completed onboarding and ran the workspace setup script, pre-commit should already be installed on your machine. You can test that it’s installed by running pre-commit -V in a terminal window. If you get a nominal return including a version number, you’re all set. If the pre-commit command is not found, please refer back to the Onboarding / Workspace Setup section of this site. If pre-commit is not installed it is important to get it installed and setup on your machine. This is a part of the workflow for developing apps in this architecture. Luckily setup is simple. Configuration . Although pre-commit is installed on your workstation, you must configure pre-commit to run for a given repository before it will begin blocking bad commits. This procedure needs to only be run once per repository, or once each time the .pre-commit-config.yaml file is changed in the repository (very infrequently). | open a terminal | install all hooks configured in .pre-commit-config.yaml cd macpro-mako pre-commit install -a . | . That’s it – after running the above commands inside the project repository, pre-comit will run the project’s configured checks before any commit. ",
    "url": "/docs/developer-guide/pre-commit/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/pre-commit/#table-of-contents"
  },"88": {
    "doc": "Pull Requests",
    "title": "Pull Requests",
    "content": " ",
    "url": "/docs/team-norms/pull-requests/",
    
    "relUrl": "/docs/team-norms/pull-requests/"
  },"89": {
    "doc": "Pull Requests",
    "title": "Introduction",
    "content": "Pull Requests (PRs) are an essential part of our software development process, enabling code review, discussion, and integration. This document outlines our team norms regarding PRs to ensure a smooth, efficient, and collaborative workflow. Above all else, please remember: your discretion and judgement supersedes this document. If you’re creating or reviewing a Pull Request that needs to break any of these norms for good reason, that’s OK. This document is the rule; exceptions are expected. ",
    "url": "/docs/team-norms/pull-requests/#introduction",
    
    "relUrl": "/docs/team-norms/pull-requests/#introduction"
  },"90": {
    "doc": "Pull Requests",
    "title": "Creating Pull Requests",
    "content": "Content and Format . | Title: The title of your PR should be carefully considered. It should be descriptive enough to convey the essence of the changes but concise enough to be easily readable. Think of the title as a brief summary that helps others quickly understand the purpose of the PR. | Follow the Template: We use a Pull Request template to standardize and streamline our PR descriptions. It is expected that all PRs will adhere to this template. | . Small, Focused Changes . | Scope: Keep PRs focused on a single task or issue to simplify review and discussion. While most pull requests will each address a single ticket, it’s OK to handle multiple Jira tickets in one PR when it makes sense. | Size: Aim for small, manageable PRs. Large PRs should be broken down into smaller parts if possible. | Formatters: Be conscious of formatting updates that your IDE may make automatically, or that you may make along the way. Sometimes small, non-functional code changes can clutter a pull request. | . ",
    "url": "/docs/team-norms/pull-requests/#creating-pull-requests",
    
    "relUrl": "/docs/team-norms/pull-requests/#creating-pull-requests"
  },"91": {
    "doc": "Pull Requests",
    "title": "Reviewing Pull Requests",
    "content": "Timeliness . | Prompt Reviews: Team members are expected to review PRs in a timely manner, typically next day or sooner. | . Constructive Feedback . | Respectful Communication: Provide constructive, respectful feedback. | Specific Comments: Use specific comments to point out issues, suggest improvements, or ask clarifying questions. | . Testing . | Test as Appropriate: The burden of full end to end testing lies with the author, our automated testing frameworks, and any manual QA process. However, reviewers should test the environment when they think it necessary, perhaps when they’ve thought of an edge case that might not be covered. | Explain your testing: The PR approach should typically include some detail around how manual tests were performed. This helps greatly in allowing reviewers to sign off without needing to test individually. | . ",
    "url": "/docs/team-norms/pull-requests/#reviewing-pull-requests",
    
    "relUrl": "/docs/team-norms/pull-requests/#reviewing-pull-requests"
  },"92": {
    "doc": "Pull Requests",
    "title": "Merging Pull Requests",
    "content": "Passing Checks . | CI/CD: Ensure all continuous integration and deployment checks have passed. | Code Quality: Code should meet our standards for quality and maintainability. | . Approval . | Minimum Approvals: PRs require a minimum number of approvals (1) from the team members before merging. | Outstanding Comments: If there are comments that ask for action or consideration to be made by the author, please address them prior to merge, regardless if you have approval. | . Merging . | Author Merges: After receiving necessary approvals, the PR author is responsible for merging the code. | Squashing Protocol: When merging into the master branch, always squash and merge. When merging into val and production, create a merge commit. | Commit Messages: We use semantic release to automatically release our product. Semantic Release looks for commit messages with special commit message syntax. Please follow this syntax when crafting your commit message. Note: GitHub will use your PR title as the default commit message when squashing; so, it’s recommended to set your PR title equal to the semantic release commit message appropriate for your changeset. | . ",
    "url": "/docs/team-norms/pull-requests/#merging-pull-requests",
    
    "relUrl": "/docs/team-norms/pull-requests/#merging-pull-requests"
  },"93": {
    "doc": "Pull Requests",
    "title": "Responsibility and Accountability",
    "content": ". | Ownership: The author is responsible for addressing feedback and ensuring the PR is ready for merging. | Collaboration: All team members share the responsibility for maintaining a high standard of code through thoughtful PR reviews. | . ",
    "url": "/docs/team-norms/pull-requests/#responsibility-and-accountability",
    
    "relUrl": "/docs/team-norms/pull-requests/#responsibility-and-accountability"
  },"94": {
    "doc": "Run Commands",
    "title": "Running Top Level Commands",
    "content": " ",
    "url": "/docs/developer-guide/run-commands/#running-top-level-commands",
    
    "relUrl": "/docs/developer-guide/run-commands/#running-top-level-commands"
  },"95": {
    "doc": "Run Commands",
    "title": "Table of contents",
    "content": ". | Overview | Commands | Options | . ",
    "url": "/docs/developer-guide/run-commands/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/run-commands/#table-of-contents"
  },"96": {
    "doc": "Run Commands",
    "title": "Overview",
    "content": "The src/run.ts file defines many helpful commands you can use to perform helpful tasks for your project. This file utilizes Yargs to provide command line interfaces to several commands for managing a serverless project. This code at src/run.ts is implementing a command-line interface (CLI) for your project. The CLI accepts various commands to manage and deploy the project. The CLI tool uses the yargs library to parse the command-line arguments. The CLI provides the following commands to use like run [command] [options] . ",
    "url": "/docs/developer-guide/run-commands/#overview",
    
    "relUrl": "/docs/developer-guide/run-commands/#overview"
  },"97": {
    "doc": "Run Commands",
    "title": "Commands",
    "content": "install installs all service dependencies. ui configures and starts a local React UI against a remote backend. deploy deploys the project. test runs all available tests. test-gui opens the unit-testing GUI for vitest. destroy destroys a stage in AWS. connect prints a connection string that can be run to ‘ssh’ directly onto the ECS Fargate task. deleteTopics deletes topics from Bigmac which were created by development/ephemeral branches. syncSecurityHubFindings syncs Security Hub findings to GitHub Issues. docs starts the Jekyll documentation site in a Docker container, available on http://localhost:4000. ",
    "url": "/docs/developer-guide/run-commands/#commands",
    
    "relUrl": "/docs/developer-guide/run-commands/#commands"
  },"98": {
    "doc": "Run Commands",
    "title": "Options",
    "content": "Each command has its own set of options that can be passed in the command line. For example, if the command deploy is used, it requires the options stage (type string, demanded option), and service (type string, not demanded option). The behavior of the command is defined by an async function, which will run the installation of all service dependencies and will execute the deployment process through the runner.run_command_and_output function with the command SLS Deploy and the options set in the command line. The same approach is used for all other commands. They all start by installing the dependencies of the services, and then perform specific tasks based on the options passed in the command line. The docs command starts a Jekyll documentation site in a Docker container. If the stop option is passed as true, it will stop any existing container. Otherwise, it will start a new container and run the documentation site at http://localhost:4000. ",
    "url": "/docs/developer-guide/run-commands/#options",
    
    "relUrl": "/docs/developer-guide/run-commands/#options"
  },"99": {
    "doc": "Run Commands",
    "title": "Run Commands",
    "content": " ",
    "url": "/docs/developer-guide/run-commands/",
    
    "relUrl": "/docs/developer-guide/run-commands/"
  },"100": {
    "doc": "Security Hub Jira Sync",
    "title": "Security Hub Jira Sync",
    "content": "Reflect our active Security Hub findings in Jira. ",
    "url": "/docs/workflows/security-hub-jira-sync/",
    
    "relUrl": "/docs/workflows/security-hub-jira-sync/"
  },"101": {
    "doc": "Security Hub Jira Sync",
    "title": "Summary",
    "content": "CMS projects deployed in AWS are required to resolve Security Hub findings according to the following schedule: . | Critical vulnerabilities within 15 days from discovery | High vulnerabilities within 30 days from discovery | Moderate (MEDIUM) vulnerabilities within 90 days from discovery | Low vulnerabilities within 365 days from discovery | . The security-hub-jira-sync workflow exists to get Security Hub findings in a project’s AWS account(s) into Jira and in front of the developers equipped to resolve them. The workflow works by running the macpro-security-hub-sync npm package on a cron, scheduled for every other hour during business hours. Please see the npm packages documentation for more details. In short: when the package is run, the master branch’s AWS account is scanned for SecHub findings, those finding types get issues created in Jira, and any resolved SecHub issues that have a Jira issue have their issue closed. ",
    "url": "/docs/workflows/security-hub-jira-sync/#summary",
    
    "relUrl": "/docs/workflows/security-hub-jira-sync/#summary"
  },"102": {
    "doc": "Security Hub Jira Sync",
    "title": "Configuration, Notes, YSK",
    "content": "Set ENABLE_SECURITY_HUB_SYNC actions variable . The security hub workflow is unique, in that it should ideally only be run by one project per AWS account. Since security hub findings are scoped to the account, and since we can have many projects deployed to a given account, multiple instances of the security hub workflow running is not ideal. It is rather harmless if two projects in the same account were both running the workflow, but it’s best avoided. To that end, a new project based off this repo will not run this workflow automatically. There is an environment variable flag that must be set before the job will run. To enable the security-hub-jira-sync workflow, set a repository variable named ‘ENABLE_SECURITY_HUB_SYNC’ to any value. The existence of the variable is what is checked, not its value. To set a GitHub actions secret, follow the same steps as creating an Actions secret, but look for a tab that says ‘Variables’. These function just like secrets in their scope, but are unencrypted. Set JIRA_USERNAME and JIRA_TOKEN as github secrets . Per the macpro-security-hub-sync docs, you’ll need to create username and token secrets for a jira user. The token is more accurately called a Personal Access Token (PAT) in Jira, and can be created by logging into Jira in a web browser and following these instructions. On MACPro, we use a service user; you probably should, too. If you’re on MACPro, you may be able to leverage our existing service user; reach out to bpaige@gswell.com or Nathan O’Donnell about possible access. Review/Update the JIRA_HOST and JIRA_PROJECT settings . In the workflow definition, there are hardcoded values for JIRA_HOST and JIRA_PROJECT. These values are what all projects on MACPro should use, so we’ve kept it simple and put them directly in the file. If you’re not on MACPro or need to change these values, make the appropriate updates to the file. Review/Update the custom fields in src/run.ts . Some Jira projects require certain fields to be set for issues to be created. This is the case with MACPro’s Jira installation. We have two fields that must be set. These fields are set in the run script. You’ll note two lines that begin with ‘customfield_’. Unfortunately, we must set the actual id and value of the custom field as Jira expects it. These ids will vary from instance to instance, even if the field name is the same. Despite our best efforts, the complexity and number of options and allowances when setting custom fields makes it incredibly difficult to have a more user friendly experience; you must find the customfield id’s for the values you must set. If you’re on MACPro and use our Jira, these are the field values you need, which map to Working Team and Product Supported. If you’re not on MACPro and using a different Jira, trial and error is usually fine; the package will surface the Jira API error as it says ‘customfield_1234 is not set, Product Supported is not set’. In other words, you can look carefully at the fail output to find the id names. Alternatively, you may also query the Jira API and be more exact about it. In any event, you probably want to update the Working Team from Platform Team to your team name. YSK the workflow only runs on the default branch . On MACPro, we typically use three separate levels of AWS accounts for each project: . | dev account: this holds the default branch (master) enviroment along with all ephemeral branches/environments. | impl account: this holds the val environment built from the val branch. | production account: this holds the production environment built from the production branch. | . Obviously, the sechub workflow needs to authenticate and talk to AWS. It gets the arn of the role to assume from a github secret. What you should know is: the scheduled/crond workflow will only kick off on the default/master branch; this is how cron’d workflows behave. This means that your workflow will only automatically manage findings in Jira for the dev aws account. If/when you’d like to scan the impl/production accounts for findings, you may manually run the security hub workflow (workflow_dispatch action) from the GitHub UI. When triggering a build, it will ask off of which branch it should run; you may select val or production as you wish. You might be wondering: why can’t we work around this? Good question; and the answer is ‘we could’. But there are bigger obstacles to automated val/production runs than the cron behavior. For instance, our OIDC role’s trust policy pattern will only allow workflows run off of the true val or production branch to assume the aws service role, respectivelly. To add to the issues, if we were to build a mechanism to trigger security hub sync off of val or production so we could assume the role, GitHub would require an administrator to approve the workflow’s access to the Environment holding the role arn before it would be allowed to continue. This is all by design, and all very good ideas. By design, having automatic and unattended builds run against val and production is not possible, or at least very difficult. In summary: you will only get automatic sec hub scanning for the dev account, which should go a long way to keeping the higher environments safe from vulnerabilities brought about by vulnerable code. Scanning impl and production must be done by manually running the workflow ad hoc, which can be accomplished through the GitHub UI. ",
    "url": "/docs/workflows/security-hub-jira-sync/#configuration-notes-ysk",
    
    "relUrl": "/docs/workflows/security-hub-jira-sync/#configuration-notes-ysk"
  },"103": {
    "doc": "Services",
    "title": "Services",
    "content": "Details on each Serverless service . The macpro-mako project is a serverless monorepo. It is, for the most part, a collection of standalone Serverless Framework micro services bound together in a repository. Loose coupling of the micro services is facilitated using one or several tools, which include CloudFormation outputs, AWS Systems Manager Parameter Store paramters, and AWS Secrets Manager stores. This section will describe each service in a high level of detail. ",
    "url": "/docs/services",
    
    "relUrl": "/docs/services"
  },"104": {
    "doc": "Subscribing to Alerts",
    "title": "Subscribing to Alerts",
    "content": "How-to to subscribe to alerts from a stage. ",
    "url": "/docs/developer-guide/subscribe-to-alerts/",
    
    "relUrl": "/docs/developer-guide/subscribe-to-alerts/"
  },"105": {
    "doc": "Subscribing to Alerts",
    "title": "Table of contents",
    "content": ". | Subscribing to Alerts . | Summary | Prerequisites: | Procedure | Notes | . | . Subscribing to Alerts . Summary . This project uses SNS for near real time alerting of application health and performance. Subscriptions to this topics are not made automatically, for a few reasons (see alerts service details). This will guide you in how to create a sbuscription. Prerequisites: . | Completed all onboarding | . Procedure . | Go to the AWS Console | Choose your region in the top right drop down. | Navigate to SNS | Click topics (see left hand side hamburger menu) and select your stage’s topic | Click add subscription and follow the prompts. | If you control the inbox for the subscription you just added, go to the inbox and click through the confirmation email from AWS. | Repeat these steps for the application’s other region. | . Notes . | None | . ",
    "url": "/docs/developer-guide/subscribe-to-alerts/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/subscribe-to-alerts/#table-of-contents"
  },"106": {
    "doc": "Team Norms",
    "title": "Team Norms",
    "content": "What’s expected and what you can expect . wip . ",
    "url": "/docs/team-norms",
    
    "relUrl": "/docs/team-norms"
  },"107": {
    "doc": "Team Introduction",
    "title": "Team Introduction",
    "content": " ",
    "url": "/docs/onboarding/team/",
    
    "relUrl": "/docs/onboarding/team/"
  },"108": {
    "doc": "Team Introduction",
    "title": "Table of contents",
    "content": ". | Meet the team | Core Work Hours | . Meet the team . A list of a few important roles and people on the project (not comprehensive!): . | Role | Description | Team Member | Email | . | Product | Responsible for project scope, direction, and delivery. | Hannah Morris | hmorris@gswell.com | . | Product | Responsible for project scope, direction, and delivery. | Erika Durant | edurant@fearless.tech | . | Tech Lead | Tooling, tech, and arch discussions and decisions. | Ben Paige | bpaige@gswell.com | . | Tech Lead | Tooling, tech, and arch discussions and decisions. | Michael Dial | mdial@gswell.com | . Core Work Hours . Core Team work hours are 10:00am - 3:00pm ET. We strive to do most work asynchronously, as we are spread across timezones and have others demands on our time. Using Pull Request reviews and comments, Slack, and email are examples of ways we try to keep things async. However, we strive to be available online for meetings and pair programming during this time window. ",
    "url": "/docs/onboarding/team/#table-of-contents",
    
    "relUrl": "/docs/onboarding/team/#table-of-contents"
  },"109": {
    "doc": "ui-infra",
    "title": "UI Infra",
    "content": " ",
    "url": "/docs/services/ui-infra/#ui-infra",
    
    "relUrl": "/docs/services/ui-infra/#ui-infra"
  },"110": {
    "doc": "ui-infra",
    "title": "Summary",
    "content": "This service provides the appropriate infrastructure for the UI application running on AWS. It creates several resources including an S3 bucket, a bucket policy, a logging bucket, a logging bucket policy, and an IAM role with permissions. ",
    "url": "/docs/services/ui-infra/#summary",
    
    "relUrl": "/docs/services/ui-infra/#summary"
  },"111": {
    "doc": "ui-infra",
    "title": "Details",
    "content": ". | AWS IAM role with permissions for CloudWatch logs and an IAM boundary policy. | Serverless plugins to help with deploying and managing the infrastructure. | Configuration settings for different stages of the infrastructure, including DNS record, CloudFront domain name, and certificates. | A set of resources to be created, including S3 buckets for hosting the UI, logging, and their policies. | . ",
    "url": "/docs/services/ui-infra/#details",
    
    "relUrl": "/docs/services/ui-infra/#details"
  },"112": {
    "doc": "ui-infra",
    "title": "Resources",
    "content": ". | An S3 bucket with server-side encryption and the ability to serve static web content. | A bucket policy that allows access to the bucket from an AWS CloudFront distribution using an Origin Access Identity (OAI). | An S3 bucket for CloudFront access logs with server-side encryption and an access policy that allows AWS root account to write logs. | A conditional statement for DNS record creation and a conditional statement for CloudFront distribution creation. | . ",
    "url": "/docs/services/ui-infra/#resources",
    
    "relUrl": "/docs/services/ui-infra/#resources"
  },"113": {
    "doc": "ui-infra",
    "title": "ui-infra",
    "content": " ",
    "url": "/docs/services/ui-infra/",
    
    "relUrl": "/docs/services/ui-infra/"
  },"114": {
    "doc": "ui",
    "title": "UI",
    "content": " ",
    "url": "/docs/services/ui/#ui",
    
    "relUrl": "/docs/services/ui/#ui"
  },"115": {
    "doc": "ui",
    "title": "Overview",
    "content": "This service deploys a static web application to an S3 bucket with a CloudFront distribution in front of it for CDN caching and performance optimization. The template uses the serverless framework and includes several plugins to help with deployment and configuration. ",
    "url": "/docs/services/ui/#overview",
    
    "relUrl": "/docs/services/ui/#overview"
  },"116": {
    "doc": "ui",
    "title": "Configuration",
    "content": "The custom section defines some custom variables, including the project name, stage, region, and CloudFormation termination protection for specific stages. The s3Sync section defines the S3 bucket to which the files will be synced, the local directory where the files will be found, and whether to delete removed files. The cloudfrontInvalidate section invalidates the CloudFront distribution cache by specifying the distribution ID and the items to invalidate. The scripts section defines a script to set environment variables during deployment, which are used to specify the API region and URL. The provider section configures the runtime environment for the Lambda functions, the AWS region, and stack tags. It does not include any IAM configuration since no Lambda functions are defined. This template is mainly focused on deploying the static web application to S3 and configuring the CloudFront distribution to serve the content. The environment variables set in the scripts section are used by the application to connect to the backend API. ",
    "url": "/docs/services/ui/#configuration",
    
    "relUrl": "/docs/services/ui/#configuration"
  },"117": {
    "doc": "ui",
    "title": "Scripts",
    "content": "There are three npm scripts that are defined in the package.json file of a project. These scripts are used to automate certain development tasks related to the project. | dev: This script runs the Vite development server. Vite is a build tool that enables fast development by providing a development server that reloads the browser quickly whenever changes are made to the code. When the dev script is run, Vite starts the development server and serves the project files on a local web server. The output of this script will typically be a URL that can be opened in a web browser to access the development server. | build: This script builds the project for production. This script first runs the TypeScript compiler (tsc) to compile the TypeScript code to JavaScript. After that, the Vite build tool is run to bundle the code and assets for production. The output of this script will typically be a set of static files that can be deployed to a web server. | preview: This script starts a Vite server that serves the production build of the project on a local web server. This is useful for testing the production build locally before deploying it to a web server. When this script is run, Vite starts the production server and serves the project files on a local web server. The output of this script will typically be a URL that can be opened in a web browser to access the production server. | . ",
    "url": "/docs/services/ui/#scripts",
    
    "relUrl": "/docs/services/ui/#scripts"
  },"118": {
    "doc": "ui",
    "title": "ui",
    "content": " ",
    "url": "/docs/services/ui/",
    
    "relUrl": "/docs/services/ui/"
  },"119": {
    "doc": "Unit Testing",
    "title": "Unit Testing",
    "content": " ",
    "url": "/docs/developer-guide/unit-testing/",
    
    "relUrl": "/docs/developer-guide/unit-testing/"
  },"120": {
    "doc": "Unit Testing",
    "title": "Table of contents",
    "content": ". | Overview | Vitest | Running Tests | . ",
    "url": "/docs/developer-guide/unit-testing/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/unit-testing/#table-of-contents"
  },"121": {
    "doc": "Unit Testing",
    "title": "Overview",
    "content": "Unit testing is done using the vitest framework. ",
    "url": "/docs/developer-guide/unit-testing/#overview",
    
    "relUrl": "/docs/developer-guide/unit-testing/#overview"
  },"122": {
    "doc": "Unit Testing",
    "title": "Vitest",
    "content": "Vitest is a unit testing framework for testing JavaScript code. It allows you to write tests in a simple and concise manner, and provides tools for running and reporting on the results of those tests. ",
    "url": "/docs/developer-guide/unit-testing/#vitest",
    
    "relUrl": "/docs/developer-guide/unit-testing/#vitest"
  },"123": {
    "doc": "Unit Testing",
    "title": "Running Tests",
    "content": "Tests can be run using the top level run commands: . | run test --stage [stage] - running all tests | run test-gui --stage [stage] - running all tests displaying results in browser ui | . ",
    "url": "/docs/developer-guide/unit-testing/#running-tests",
    
    "relUrl": "/docs/developer-guide/unit-testing/#running-tests"
  },"124": {
    "doc": "Updating Diagram",
    "title": "Updating Architecture Diagram",
    "content": " ",
    "url": "/docs/developer-guide/update-diagram/#updating-architecture-diagram",
    
    "relUrl": "/docs/developer-guide/update-diagram/#updating-architecture-diagram"
  },"125": {
    "doc": "Updating Diagram",
    "title": "Table of contents",
    "content": ". | Purpose | What is eraser.io | Updating Architectural Diagram | . ",
    "url": "/docs/developer-guide/update-diagram/#table-of-contents",
    
    "relUrl": "/docs/developer-guide/update-diagram/#table-of-contents"
  },"126": {
    "doc": "Updating Diagram",
    "title": "Purpose",
    "content": "The goal of this document is to guide users on how to update the maco architectural diagram using eraser.io . ",
    "url": "/docs/developer-guide/update-diagram/#purpose",
    
    "relUrl": "/docs/developer-guide/update-diagram/#purpose"
  },"127": {
    "doc": "Updating Diagram",
    "title": "What is eraser.io",
    "content": "Eraser is a tool that enables developers to create docs and diagrams at the speed of thought via a minimal UI, keyboard-driven flows, markdown, and diagram-as-code. To access the editable version of the MAKO use the link provided below Architecture Diagram . ",
    "url": "/docs/developer-guide/update-diagram/#what-is-eraserio",
    
    "relUrl": "/docs/developer-guide/update-diagram/#what-is-eraserio"
  },"128": {
    "doc": "Updating Diagram",
    "title": "Updating Architectural Diagram",
    "content": ". | Go to the link provided above | Make desired changes to the diagram using the tools provided by eraser.io | Copy diagram . | Select the entire diagram (highlight all images) | Right click on any area of the diagram while highlighted | Select “Copy/Past As” and select “Copy As SVG” | . | Go to docs &gt; assets &gt; diagram-twomac.svg . | Replace the copied item eraser.io with the item in line 4 to 6 | . | . ",
    "url": "/docs/developer-guide/update-diagram/#updating-architectural-diagram",
    
    "relUrl": "/docs/developer-guide/update-diagram/#updating-architectural-diagram"
  },"129": {
    "doc": "Updating Diagram",
    "title": "Updating Diagram",
    "content": " ",
    "url": "/docs/developer-guide/update-diagram/",
    
    "relUrl": "/docs/developer-guide/update-diagram/"
  },"130": {
    "doc": "Webforms",
    "title": "Webforms",
    "content": " ",
    "url": "/docs/webforms/",
    
    "relUrl": "/docs/webforms/"
  },"131": {
    "doc": "Webforms",
    "title": "Table of contents",
    "content": ". | Purpose of this view | What are onemac webforms | Things to note | Key structuring of Webforms: | . ClICK HERE to view documentation for individual webforms for the macpro-mako project. ",
    "url": "/docs/webforms/#table-of-contents",
    
    "relUrl": "/docs/webforms/#table-of-contents"
  },"132": {
    "doc": "Webforms",
    "title": "Purpose of this view",
    "content": "The goal of the webforms view is to be able to view all possible form fields as they relate to the data collected. We try to associate the “name” of the field, which is the key of the data as it is collected and stored, to the prompt and label of the question the user is presented with. ",
    "url": "/docs/webforms/#purpose-of-this-view",
    
    "relUrl": "/docs/webforms/#purpose-of-this-view"
  },"133": {
    "doc": "Webforms",
    "title": "What are onemac webforms",
    "content": "These webforms are a replacement for the pdf form uploads previously used in mmdl, etc. These are dynamicly generated forms from a single document that represent the shape of a form and version. There three pieces to this puzzle. | The form schema itself. this is the shape of a form that is delivered by the /forms endpoint to the frontend ui. | The form genereator. This is a collection of ui react components and react-hook-form methods that generate the ui from the form schema and render and validate the form presented to the user. | The data. The data collected is added along with its metadata to its parent record. This data can be used to view/approve/edit the form later by admins or other users with appropriate permissions. | . ",
    "url": "/docs/webforms/#what-are-onemac-webforms",
    
    "relUrl": "/docs/webforms/#what-are-onemac-webforms"
  },"134": {
    "doc": "Webforms",
    "title": "Things to note",
    "content": "Some of the fields listed in these webforms are FieldArrays. This means that they are arrays of values that contain groups of like values (think an array of identicaly typed objects). The items within those arrays will have Parent values to indicate which parent they belong to. ",
    "url": "/docs/webforms/#things-to-note",
    
    "relUrl": "/docs/webforms/#things-to-note"
  },"135": {
    "doc": "Webforms",
    "title": "Key structuring of Webforms:",
    "content": "For our webforms, we are in communication with data connect on our we should pass our json data to their team. We have gone with this approach to help streamline our data, but also make it as easy as possible to hunt down questions if items are added or changed. Our “name” keys are made up of for different parts. Our agreed upon key structures for the “name” property of form field sections are as follows: . The Four Parts to the name field are: . | form name ‘-‘ instead of dots | section title separated by “-” shortened if possible | input id/label as close and short as possible with only ‘-‘ as separators | input type is usually the (rhf) value text/textarea/array/checkgroup/upload/radiogroup/select | . [form-name]_[section-title]_[question-id]_[input] . Example: “abp1_pop-id_abp-pop-name_input” . It is imperative that this pattern be followed in all forms. ",
    "url": "/docs/webforms/#key-structuring-of-webforms",
    
    "relUrl": "/docs/webforms/#key-structuring-of-webforms"
  },"136": {
    "doc": "GitHub Workflows",
    "title": "GitHub Actions",
    "content": "Details on our workflows! . GitHub Actions is a CI/CD platform that can be used in any GitHub repository. It’s organized at the highest level into workflows. Each workflow is defined in its own file located in .github/workflows/. Each workflow file defines one or more jobs to run and when to run them. This section will detail each of the project’s workflows. Some may require a fair amount of detail, others may be simple and obvious. For sake of portability, each will be documented in its own file in this documentation site. If you see any gaps, please let us know! GitHub Actions is really the production floor of our product, and its important to have complete documentation. ",
    "url": "/docs/workflows#github-actions",
    
    "relUrl": "/docs/workflows#github-actions"
  },"137": {
    "doc": "GitHub Workflows",
    "title": "GitHub Workflows",
    "content": " ",
    "url": "/docs/workflows",
    
    "relUrl": "/docs/workflows"
  },"138": {
    "doc": "Workspace Setup",
    "title": "Workspace Setup",
    "content": "Before you begin development, it’s important to configure your workstation properly. This section will give you an overview of what tools are installed and get you bootstrapped. ",
    "url": "/docs/onboarding/workspace-setup/",
    
    "relUrl": "/docs/onboarding/workspace-setup/"
  },"139": {
    "doc": "Workspace Setup",
    "title": "Table of contents",
    "content": ". | Development Tool list | Install Rosetta 2 (Apple Silicon only) | Install XCode Command Line Tools | Run the setup.sh script | Clone the repository | Configure AWS CLI | Now what? | . ",
    "url": "/docs/onboarding/workspace-setup/#table-of-contents",
    
    "relUrl": "/docs/onboarding/workspace-setup/#table-of-contents"
  },"140": {
    "doc": "Workspace Setup",
    "title": "Development Tool list",
    "content": "This is a static list of tools that should be pre-installed to support all Developer Guide. Please understand that the installation of most of these tools is automated, which will be discussed in the next section. This serves as a general overview of what will be installed. | Tool | Version | Required | . | MacOS | 10.15+ | Yes | . | Rosetta 2 (Apple Silicon only) | 2 | Yes | . | XCode Command Line Tools | 2392 | Yes | . | NodeJS | 14.x | Yes | . | Node Version Manager (NVM) | 0.39.1 | Yes | . | Bun | 1.1.20 | Yes | . | Direnv | 2.31.0 | Yes | . | AWS CLI | 2.x | Yes | . | AWS CLI Sessions Manager Plugin | 1.2.312.0 | Yes | . | jq | jq-1.6 | Yes | . | awslogs | 1.2.312.0 | Optional | . | Git | 2.x | Yes | . ",
    "url": "/docs/onboarding/workspace-setup/#development-tool-list",
    
    "relUrl": "/docs/onboarding/workspace-setup/#development-tool-list"
  },"141": {
    "doc": "Workspace Setup",
    "title": "Install Rosetta 2 (Apple Silicon only)",
    "content": "If you’re on a newer model Mac with an Apple ARM based chip (M1 series), you need to install Rosetta 2. Follow this Apple Support guide to install Rosetta 2. ",
    "url": "/docs/onboarding/workspace-setup/#install-rosetta-2-apple-silicon-only",
    
    "relUrl": "/docs/onboarding/workspace-setup/#install-rosetta-2-apple-silicon-only"
  },"142": {
    "doc": "Workspace Setup",
    "title": "Install XCode Command Line Tools",
    "content": "Follow the installation guide. ",
    "url": "/docs/onboarding/workspace-setup/#install-xcode-command-line-tools",
    
    "relUrl": "/docs/onboarding/workspace-setup/#install-xcode-command-line-tools"
  },"143": {
    "doc": "Workspace Setup",
    "title": "Run the setup.sh script",
    "content": "Once you reach this step, the remainder of the tools you’ll need can be installed with a simple script. | Download and save the setup.sh script somewhere accessible on your computer. Your Downloads folder, which is probably your default download location, is fine. | Open a terminal | Run the setup.sh script sh ~/Downloads/setup.sh . Please note that you may be prompted to input your OS user’s password, as some installation steps require higher priveleges. | . After successfully running the setup script, close all open terminals. The script adds PATH modifications to your .zshrc file, and that file will only take affect when starting a new terminal session. To ensure you immediately have the PATH modifications available, it’s easiest to simply close all terminal windows now. You may reopen the terminal at any time afterwards. ",
    "url": "/docs/onboarding/workspace-setup/#run-the-setupsh-script",
    
    "relUrl": "/docs/onboarding/workspace-setup/#run-the-setupsh-script"
  },"144": {
    "doc": "Workspace Setup",
    "title": "Clone the repository",
    "content": "Now that you have all prerequisites installed, it’s time to get the code base. This will require your git repo access request to have been completed. | Configure your GitHub user with an SSH key, and add it to your ssh-agent. Help can be found here. Using https and personal access tokens, instead of SSH and a key, is possible but discouraged. | Clone the repository git clone git@github.com:Enterprise-CMCS/macpro-mako.git . | . ",
    "url": "/docs/onboarding/workspace-setup/#clone-the-repository",
    
    "relUrl": "/docs/onboarding/workspace-setup/#clone-the-repository"
  },"145": {
    "doc": "Workspace Setup",
    "title": "Configure AWS CLI",
    "content": "This step requires that your request for AWS access has been completed. If it hasn’t, you’ll need to wait, then return here. | Login to the AWS Console. | Create an AWS Access Key Id and Secret Access Key for your IAM user. Detailed instructions are available. | Configure the AWS CLI with valid credentials. Background and instructions can be found here. Running aws configure and following the prompts should suffice, though. | . ",
    "url": "/docs/onboarding/workspace-setup/#configure-aws-cli",
    
    "relUrl": "/docs/onboarding/workspace-setup/#configure-aws-cli"
  },"146": {
    "doc": "Workspace Setup",
    "title": "Now what?",
    "content": "Nice job! If you’ve successfully stepped through this document, you should be entirely ready to start active development on the macpro-mako project. If you’ve had errors along the way, that’s OK! We’re here to help. If you’ve successfully joined our Slack channel, feel free to post there. Else, please send an email to mdial@collabralink.com explaining the issue. We will get back to you ASAP. ",
    "url": "/docs/onboarding/workspace-setup/#now-what",
    
    "relUrl": "/docs/onboarding/workspace-setup/#now-what"
  }
}
